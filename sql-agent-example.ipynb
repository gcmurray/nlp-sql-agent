{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b8a02c",
   "metadata": {},
   "source": [
    "## Core Components\n",
    "First we build our llm, our SQL database, our embedding model, and our document vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e49b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import add_messages, START, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.config import get_stream_writer\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0591e9b",
   "metadata": {},
   "source": [
    "### Build the Main Components\n",
    "Create the LLM, the SQL database, the embeddings model, the RAG vector store, and the RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158d268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/WVU-AD/gcm0011/Research/nlp-sql-agent/agent-env/lib/python3.12/site-packages/langchain_experimental/sql/base.py:77: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# chat model\n",
    "llm = ChatOllama(model=\"mistral:latest\")\n",
    "\n",
    "# SQL database ---------------------------------------\n",
    "db = SQLDatabase.from_uri(\n",
    "    f\"postgresql+psycopg2://postgres:password@localhost:5432/ta_database\",\n",
    ")\n",
    "\n",
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# embeddings for vector store retriever -----------------\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# the vector store, course content\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"asm_z80\",\n",
    "    embedding_function=embeddings,\n",
    "    host=\"localhost\",\n",
    ")\n",
    "\n",
    "rag_prompt = PromptTemplate(template=\"\"\"You are an assistant for gathering relevant course material.\n",
    "                Repeat the If the content is empty, answer ONLY a single word, 'NONE'.\n",
    "                Question: {question}\n",
    "                Content: {context}\n",
    "                Answer: \"\"\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58399c",
   "metadata": {},
   "source": [
    "### Build the SQL Chain\n",
    "Create the SQL chain for getting grade information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46a6d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196101/2506761555.py:5: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator(\"query\")\n"
     ]
    }
   ],
   "source": [
    "# Define schema for validation\n",
    "class SQLQuery(BaseModel):\n",
    "    query: str = Field(description=\"A valid SQL query targeting the student/assignment/grade database\")\n",
    "\n",
    "    @validator(\"query\")\n",
    "    def must_start_with_select(cls, v):\n",
    "        if not v.strip().lower().startswith(\"select\"):\n",
    "            raise ValueError(\"Only SELECT queries are allowed.\")\n",
    "        return v\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=SQLQuery)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a helpful SQL generator.\\n\\n\"\n",
    "        \"User request: {question}\\n\\n\"\n",
    "        \"Generate a valid SQL SELECT query for the students/assignments/grades schema.\\n\"\n",
    "        \"{format_instructions}\"\n",
    "    ),\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "sql_chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0228b13",
   "metadata": {},
   "source": [
    "## Setup The State for our Agent\n",
    "This will create the parts of the agent state we want to track and manipulate, including messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d18f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # here we can also put state tracking on status, if we are quizzing, etc.\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    init_question: str\n",
    "    sql_query: str # the sql code created by llm\n",
    "    sql_answer: str # the sql result\n",
    "    rag_answer: str #the rag response\n",
    "    rag_attempts: int\n",
    "    route_attempts: int\n",
    "    route: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37911f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1238369200.py, line 103)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif docs\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "# Node for converting the user query into a SQL command.\n",
    "def init_node(state: State) -> dict:\n",
    "    init_msg = \"Please introduce yourself, your job, and the course description.\"\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if msg.type == \"human\":\n",
    "            init_msg = msg.content\n",
    "            break\n",
    "    return {\"init_question\": init_msg, \"sql_query\": None, \n",
    "            \"sql_answer\": None, \"rag_answer\": None,\n",
    "            \"rag_attempts\": 0, \"route_attempts\": 0, \n",
    "            \"route\": None}\n",
    "\n",
    "def sql_generator_node(state: State) -> dict:\n",
    "    # latest user message\n",
    "    user_msg = state[\"init_question\"]\n",
    "\n",
    "    # run through LLM + parser\n",
    "    sql_query = sql_chain.invoke({\"question\": user_msg})\n",
    "\n",
    "    return {\"sql_query\": sql_query.query}\n",
    "\n",
    "# Node for executing the SQL command that was generated.\n",
    "def sql_execution_node(state: State) -> dict:\n",
    "    result = db_chain.run(state[\"sql_query\"])\n",
    "    return {\"messages\": [(\"tool\", f\"Query result: {result}\")], \"sql_answer\": result}\n",
    "\n",
    "def response_node(state: State):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated SQL and RAG responses, and initial question\n",
    "    sql_answer = state[\"sql_answer\"]\n",
    "    rag_answer = state[\"rag_answer\"]\n",
    "    convo_messages = []\n",
    "\n",
    "    # get the messages.\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        convo_messages.append(message)\n",
    "    \n",
    "    system_message_content = (\n",
    "        \"You are a teacher, answering students questions.\"\n",
    "        \"Use the following information to answer the questions. \"\n",
    "        \"If you don't know the answer, say that you don't know. \"\n",
    "        \"\\n\\n\"\n",
    "        f\"{sql_answer}\\n\\n\"\n",
    "        f\"{rag_answer}\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    \n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "router_prompt = PromptTemplate.from_template(\n",
    "    \"Answer based on the following context:\\n\\n\"\n",
    "    \"Messages: {messages}\\n\\n\"\n",
    "    \"Answer with ONLY one word:\"\n",
    "    \"'done' if all questions have been answered,\"\n",
    "    \"'sql' if there are unanswered questions about scores or students, or assignments, or grades,\"\n",
    "    \"or 'course' if there are unaswered questions about course material,\"\n",
    ")\n",
    "\n",
    "router_chain = router_prompt | llm\n",
    "\n",
    "def router(state: State) -> dict:\n",
    "    if state[\"route_attempts\"] > 3:\n",
    "        return {\"route\": \"done\"}\n",
    "    \n",
    "    user_msg = state[\"init_question\"]\n",
    "    recent_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        recent_messages.append(message)\n",
    "    \n",
    "    recent_messages = recent_messages[::-1]\n",
    "    if len(recent_messages) > 10:\n",
    "        recent_messages = recent_messages[::-10]\n",
    "    \n",
    "    context = \"\\n\\n\".join(message.content for message in recent_messages)\n",
    "\n",
    "    route = router_chain.invoke({\"messages\": context}).content.strip().lower()\n",
    "    if \"sql\" in route:\n",
    "        return {\"route\": \"generate_sql\", \"route_attempts\": state.get(\"route_attempts\", 0) + 1}\n",
    "    elif \"course\" in route:\n",
    "        return {\"route\": \"course\", \"route_attempts\": state.get(\"route_attempts\", 0) + 1}\n",
    "    else:\n",
    "        return {\"route\": \"done\", \"route_attempts\": 0}\n",
    "\n",
    "def routing_function(state: State) -> dict:\n",
    "    return state[\"route\"].strip().lower()\n",
    "\n",
    "def rag_retrieve_node(state: State) -> dict:\n",
    "    user_msg = state[\"init_question\"]\n",
    "    #messages = state[\"messages\"]\n",
    "    #messages = \"\\n\\n\".join(message.content for message in messages)\n",
    "    docs = compression_retriever.get_relevant_documents(query = user_msg)\n",
    "    \n",
    "    if docs is None:\n",
    "        return {\"answer\": None}\n",
    "    \n",
    "    docs_string = \"\\n\\n\".join(doc.page_content for doc in docs) # TODO: make Course answer a summary?\n",
    "    return {\"rag_answer\": docs_string, \"messages\": [(\"tool\", f\"Course answer: {docs_string}\")]}\n",
    "\n",
    "rephrase_prompt = PromptTemplate.from_template(\n",
    "    \"The user asked:\\n\\n{question}\\n\\n\"\n",
    "    \"This did not retrieve relevant course material. \"\n",
    "    \"Rephrase it in a clearer way that might match the knowledge base.\"\n",
    ")\n",
    "rephrase_chain = rephrase_prompt | llm\n",
    "\n",
    "def rag_rephrase_node(state: State) -> dict:\n",
    "    user_msg = state[\"init_question\"]\n",
    "    new_question = rephrase_chain.invoke({\"question\": user_msg}).content\n",
    "    return {\n",
    "        \"messages\": [(\"human\", new_question)],\n",
    "        \"rag_attempts\": state.get(\"rag_attempts\", 0) + 1,\n",
    "    }\n",
    "\n",
    "def rag_fallback_node(state: State) -> dict:\n",
    "    return {\"messages\": [(\"ai\", \"Sorry, I couldn't find relevant course material even after rephrasing.\")]}\n",
    "\n",
    "def rag_router(state: State) -> str:\n",
    "    if state.get(\"answer\"):\n",
    "        return \"found\"  # stop\n",
    "    elif state.get(\"rag_attempts\", 0) >= 3:\n",
    "        return \"fallback\"\n",
    "    else:\n",
    "        return \"rephrase\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_builder = StateGraph(State)\n",
    "\n",
    "rag_builder.add_node(\"retrieve\", rag_retrieve_node)\n",
    "rag_builder.add_node(\"rephrase\", rag_rephrase_node)\n",
    "rag_builder.add_node(\"fallback\", rag_fallback_node)\n",
    "\n",
    "rag_builder.set_entry_point(\"retrieve\")\n",
    "\n",
    "rag_builder.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    rag_router,\n",
    "    {\n",
    "        \"found\": END,\n",
    "        \"rephrase\": \"rephrase\",\n",
    "        \"fallback\": \"fallback\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Loop rephrased query back to retrieve\n",
    "rag_builder.add_edge(\"rephrase\", \"retrieve\")\n",
    "\n",
    "rag_graph = rag_builder.compile()\n",
    "\n",
    "def course_node(state: State) -> dict:\n",
    "    return rag_graph.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794297d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"init\", init_node)\n",
    "builder.add_node(\"router\", router)\n",
    "builder.add_node(\"generate_sql\", sql_generator_node)\n",
    "builder.add_node(\"execute_sql\", sql_execution_node)\n",
    "builder.add_node(\"course\", course_node)\n",
    "builder.add_node(\"response\", response_node)\n",
    "\n",
    "# Entry\n",
    "builder.set_entry_point(\"init\")\n",
    "\n",
    "# Conditional edges from router\n",
    "builder.add_conditional_edges(\n",
    "    \"router\",\n",
    "    routing_function,  # function returning the next node name\n",
    "    {\n",
    "        \"generate_sql\": \"generate_sql\",\n",
    "        \"course\": \"course\",\n",
    "        \"done\": \"response\"\n",
    "    },\n",
    ")\n",
    "\n",
    "# Chain SQL path\n",
    "builder.add_edge(\"init\", \"router\")\n",
    "builder.add_edge(\"generate_sql\", \"execute_sql\")\n",
    "builder.add_edge(\"execute_sql\", \"router\")\n",
    "builder.add_edge(\"course\", \"router\")\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcee791",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What is the course lesson for day 7?\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"greg\"}}\n",
    "\n",
    "for chunk in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": input_message}]}, stream_mode=\"updates\", config=config):\n",
    "    for state_key, state_value in chunk.items():\n",
    "        if state_key == \"messages\":\n",
    "            state_value[-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
